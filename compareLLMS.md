# Features

- open source/close
- chat / reasoning / hybrid
- release date & knowledge cut off
- more paramters generally better
- training tokens -(how much data was used to train this one)
- context window .... all the propmpts it can hold in memory.

Google for the Model Board.

Inference cost: (local model is free, hammering my comp.)
Cost of extra training 
Latency to the first token to reply quickly

Getting better performance is not only related to how well it was trained.
Also how well you infer from it. - with techniques like RAG. (Chinchilla scaling) - Given the same model, same architecture, same training techniques -to  double the paramters you'll need twice as much training data.

GPQA - google proof Question Answer (Not even with google help can save us)
MMLUORO - language understanding (broken and ambiguous test)
AIME
LiveCodeBench
MuSR
HLE (humanities last exam)

who has the means, motive and opportunity?

its been a hefty move since then... 'll let you ponder.
i felt the pressure of your eye roll

issues:

1. training data contamination (future models seems to know ans to qs)
2. knows its being evaluated

- evaluate, threats, opportunity, strategy.



BUSINESS Challenge:
1. Convert code from python to C++ for better performance.


